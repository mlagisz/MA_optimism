---
title: "preprocessing"
author: "ML"
date: "04/01/2020"
output: html_document
---

# MA of cognitive bias (optimism) in animal studies.

## STEP_01 - preprocessing data

### Setup R
```{r setup}
options(scipen=100)

#install.packages("car", "meta", "metafor", "ape", "dplyr", "ggplot2", "scales", "cowplot")
#install.packages("devtools","fulltxt")
#library(readxl)
#update.packages("fulltxt")
#library(fulltext)
#devtools::install_github("ropensci/rotl", dependencies = TRUE, build_vignette=TRUE)
#install.packages("rotl")
#library(rotl)
#library(ape)
#library(phytools)
#library(dplyr)
#library(tidyr)
#library(ggplot2)
#library(scales)
#library(easyGgplot2)
#library(cowplot)

pacman::p_load(tidyverse, magrittr, fulltext, rotl, ape, dplyr, tidyr, readxl, janitor) #load packages
source("./functions/functions.R") #load custom functions

#writeLines(capture.output(sessionInfo()), "./log_sessioninfo/sessionInfo.txt") #save session info into a text file
writeLines(capture.output(sessionInfo()), paste0("./log_sessioninfo/sessionInfo_STEP01_", format(Sys.time(), "%d-%b-%Y %H.%M"), ".txt")) #save session info into a text file with timestamp in the file name
```

### Load MA dataset
                                        
```{r load MA extractions dataset, eval=TRUE}                                          
#load data from Excel file:
dat <- read_excel("./data/MA_dataset_2019_updated.xlsx", sheet = 1, range="A1:BH520")
dim(dat) #519 60
names(dat)
tail(dat)
str(dat) 

#exclude data points marked as "Y" in the Exclude column: 
dat %>% tabyl(Exclude) #60 rows to be removed (usually missing values at cues or wrong experimental design)
dat %>% filter(is.na(Exclude)) -> dat
dim(dat) #459 60
dat %>% dplyr::select(-c(Exclude)) -> dat#get rid of empty column

#check for missing data:
unlist(lapply(dat, function(x) sum(is.na(x)))) #see how many NA per numerical column, no missing means or SD or SE or N

get_dupes(dat, EffectID) #there are 5 pairs of duplicated EffectID, fix
dat$EffectID <- as.factor(paste("ES", 1:nrow(dat), sep="_")) #fix column with ES ID (unique ID for effect sizes)

dat <- droplevels(dat) #get rid of redundant factor levels
```

### Initial data checks

```{r data check, eval=TRUE}  
#study designs
dat %>% tabyl(WithinBetween) #mostly between
dat %>% tabyl(StudyDesign, WithinBetween)

#which means are already logit-transformed?
table(dat$DataScale) #10 data points on logit scale
dat[which(dat$DataScale == "logit"), ]

#find rows with means or SD = 0 for natural scale proportion data
dat[which(dat$Better <= 0 & dat$DataScale == "natural"), ] #2 rows
dat[which(dat$BetterSD <= 0 & dat$DataScale == "natural"), ] #5 rows

sort(dat[which(dat$Better < 0.05 & dat$DataScale == "natural"), ]$Better) #see the smallest value after 0
dat$Better[which(dat$Better <= 0 & dat$DataScale == "natural")] <- 0.01 #substitute the 0 values
sort(dat[which(dat$BetterSD < 0.05 & dat$DataScale == "natural"), ]$BetterSD) #see the smallest value after 0
dat$BetterSD[which(dat$BetterSD <= 0 & dat$DataScale == "natural")] <- 0.01 #substitute the 0 values

dat[which(dat$Worse <= 0 & dat$DataScale == "natural"), ] #10 rows
dat[which(dat$WorseSD <= 0 & dat$DataScale == "natural"), ] #15 rows

sort(dat[which(dat$Worse < 0.05 & dat$DataScale == "natural"), ]$Worse) #see the smallest value after 0
dat$Worse[which(dat$Worse <= 0 & dat$DataScale == "natural")] <- 0.01 #substitute the <=0 values 
sort(dat[which(dat$WorseSD < 0.05 & dat$DataScale == "natural"), ]$WorseSD) #see the smallest value after 0
dat$WorseSD[which(dat$WorseSD <= 0 & dat$DataScale == "natural")] <- 0.01 #substitute the <=0 values

#find rows with means proportions > 11 (more than 100%) for the proportion data subset
dat[which(dat$Better >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ] #3 rows
dat[which(dat$Worse >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ] #5 rows

sort(dat[which(dat$Worse > 0.9 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ]$Worse) #see the largest value before 1
dat$Worse[which(dat$Worse >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion")] <- 0.99 #substitute the >1 values
sort(dat[which(dat$Better > 0.9 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ]$Better) #see the largest value before 1
dat$Better[which(dat$Better >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion")] <- 0.99 #substitute the >1 values


#find rows with means proportions <= 0 (less than 0%) for the proportion data subset
dat[which(dat$Better <= 0 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ] #2 rows
dat[which(dat$Worse <= 0 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ] #5 rows

sort(dat[which(dat$Worse < 0.05 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ]$Worse) #see the smallest value after 0
dat$Worse[which(dat$Worse <= 0 & dat$DataScale == "natural" & dat$MeasureType == "proportion")] <- 0.01 #substitute the <=0 values 
sort(dat[which(dat$Better < 0.05 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ]$Better) #see the smallest value after 0
dat$Better[which(dat$Better <= 0 & dat$DataScale == "natural" & dat$MeasureType == "proportion")] <- 0.01 #substitute the <=0 values 
```


### Caluclate effect sizes (ES) Hedges g

The outcome measures are either letencies or proportions (percent or logit). Need to use different functions for calculating effect sizes.

```{r calculate ES, eval=TRUE}  
 dat %>% tabyl(MeasureType, DataScale) # 201 "latency" (natural), 248 "proportion", and 10 "logit proportion"

#subset the original dataframe by outcome measure type and recalculate means and variances to the same scale using custom functions for specific data types and scales

## latency data subset
 dat_lat <- dplyr::filter(dat, MeasureType == "latency") #subset latency data
 dim(dat_lat) #201 59
 dat_lat2 <- dat_lat %>% calc_ES_latency(Worse, WorseSD, WorseN, Better, BetterSD, BetterN, WithinBetween, adjusted=TRUE, type="lnorm") #calcualte ES (d = Hedges g) and reverse its sign
 
 dim(dat_lat2) #201 67
 hist(dat_lat2$d)
 hist(dat_lat2$Vd)
 #dplyr::filter(dat_lat2, d < -8)[, 1:6]
 #dplyr::filter(dat_lat2, d < -8)[, 46:67]
 
## logit of proportion data subset
 dat_prop_logit <- dplyr::filter(dat, MeasureType == "proportion" & DataScale == "logit") #subset proportion data (percentage) on logit scale
 dim(dat_prop_logit) #10 59
 
 hist(dat_prop_logit$Better)
 hist(dat_prop_logit$BetterSD)
 hist(dat_prop_logit$Worse)
 hist(dat_prop_logit$WorseSD)
 
 table(dat_prop_logit$WithinBetween) #by stydy design type: 6 between
 dat_prop_logit2 <- dat_prop_logit %>% calc_ES_proportion(Worse, WorseSD, WorseN, Better, BetterSD, BetterN, WithinBetween, adjusted=TRUE, type="logit") #calcualte ES (d = Hedges g)

 dim(dat_prop_logit2) #10 67
 hist(dat_prop_logit2$d)
 hist(dat_prop_logit2$Vd)
  
## percent data subset
 dat_prop_pct <- dplyr::filter(dat, MeasureType == "proportion" & DataScale == "natural") #subset proportion data (percentage) on natural scale
 dim(dat_prop_pct) #248 59
 
 hist(dat_prop_pct$Better)
 hist(dat_prop_pct$BetterSD) 
 hist(dat_prop_pct$Worse) 
 hist(dat_prop_pct$WorseSD) 

 table(dat_prop_pct$WithinBetween) #by stydy design type: between, within
 dat_prop_pct2 <- dat_prop_pct %>% calc_ES_proportion(Worse, WorseSD, WorseN, Better, BetterSD, BetterN, WithinBetween, adjusted=TRUE, type="proportion") #calcualte ES (d = Hedges g)
 
 dim(dat_prop_pct2) #248 67
 hist(dat_prop_pct2$d)
 hist(dat_prop_pct2$Vd)

#View(dplyr::filter(dat2, d > 4)) #look at effect sizes > 4  checked against the original papers
#View(dplyr::filter(dat2, d < (-2))) #look at effect sizes <  -4  checked agains the original papers

 #very large negative ES - check:
 dplyr::filter(dat_prop_pct2, d < -3)[, 46:67] #3
 dplyr::filter(dat_prop_pct2, d < -3)[, 1:10] #3
 #very large positive ES - check:
 dplyr::filter(dat_prop_pct2, d > 3)[, 46:67] #4
 dplyr::filter(dat_prop_pct2, d > 3)[, 1:10] #4
 
## merge back the subsets
dat2 <- rbind(dat_lat2, dat_prop_logit2, dat_prop_pct2)
names(dat2)
hist(dat2$d)
hist(dat2$Vd)

dat2$Species_Latin <- as.character(dat2$ScientificName) #Make new column in the data frame and adjust species names
unique(dat2$Species_Latin) #22 species
write.csv(dat2, "data/MA_dataset_with_ES.csv", row.names = FALSE)
```


############## TREE 

Prepare phyogenetic tree for the included species  

```{r  tree, eval=TRUE}
#"The function tnrs_match_names returns a data frame that lists the Open Tree identifiers as
#well as other information to help users ensure that the taxa matched are the correct ones"

#names(dat2)
dat2 %>% tabyl(ScientificName)
#unique(dat2$Species_Latin)

species_list <- as.character(unique(dat2$Species_Latin))
taxa <- tnrs_match_names(names = species_list) #call rotl function to find these species on OTL
taxa #Capra hircus come with an extra text in the brackets, Drosophila melanogaster comes as Sporophila melanogaster with ott_id 128440, which is a wrong one (for the birs - black-bellied seedeater!)
tree <- tol_induced_subtree(ott_ids= c(taxa[["ott_id"]], 505714)) #added correct D.melanogaster OTT: 128440 (was not fonding it correctly)
str(tree) # no branch lengths
is.binary.tree(tree) #TRUE 
tree$tip.label #23 tree tips /species
plot(tree, cex=.8, label.offset =.1, no.margin = TRUE)
tree$node.label <- "" # remove node labels
tree$tip.label <- gsub("_ott.*","", tree$tip.label) #clean up tip labels - remove ottID
tree$tip.label <- gsub("_\\(.*)","", tree$tip.label) #clean up tip labels - remove extra text in brackets
tree <- drop.tip(tree, "Sporophila_melanogaster")
plot(tree, cex=.8, label.offset =.1, no.margin = TRUE)
tree <- collapse.singles(tree)
intersect(unique(dat2$Species_Latin), tree$tip.label) #22 are matching
setdiff(unique(dat2$Species_Latin), tree$tip.label) #0 if all matching

### computing branch lengths
tree_bl <- compute.brlen(tree)
plot(tree_bl)
is.binary.tree(tree_bl)
is.ultrametric(tree_bl)
str(tree_bl)
#if you need to remove some species from the tree later (due to working on the subset of the data), use drop.tip function, for example:
#tree_noInsect <- drop.tip(tree, "Bombus_terrestris_audax") # remove "Bombus_terrestris_audax" from the tree

write.tree(tree_bl,file="data/tree_all.tre")
#tree <- read.tree(file="data/tree_all.tre")
```



############################## NEW adding variables for creating subsets

```{r subsets, eval=TRUE}
#dat2 <- read.csv("data/MA_dataset_with_ES.csv")

#table(dat2$ExperimentID)
#table(dat2$ExperimentID, dat2$Sex)
#table(dat2$ExperimentID, dat2$Sex, dat2$MeasureType) 
#table(table(dat$ExperimentID, dat2$Sex, dat2$MeasureType))
table(table(dat2$ExperimentID, dat2$Sex, dat2$MeasureType, dat2$ComparisonCategory)) #use this to get all the sets of points from the same curve (1-5)

#tbl_df(dat2) %>% group_by(ExperimentID, MeasureType, GroupID, ComparisonCategory, Sex) # sets of points are set by indices stored in a list
dat2$unique <- as.factor(paste(paste0(dat2$ExperimentID, dat2$MeasureType, dat2$GroupID, dat2$ComparisonCategory, dat2$Sex))) # sets of points are set by unique indices stored in a column
uni <- levels(dat2$unique)
#uni

#create a new column with absoulte avlue of each effect size
dat2$abs_d <- abs(dat2$d)

#create new empty columns
dat2$max_ES <- NA
dat2$biggest_ES <- NA
dat2$biggest_meandir <- NA

#fill in new columns and create new data frame 

for (i in 1:length(uni)){
  tempdat <- dat2[dat2$unique == uni[i],] #subset by unique==uni

  
  max_es <- max(tempdat$d) #find max value of ES (in positive direction) from each curve
  max_es_row <- which(tempdat$d == max_es)[1]
  max_es_ScalePoint = tempdat$ScalePoint[max_es_row]
  tempdat$max_ES[tempdat$ScalePoint == max_es_ScalePoint[1]] <- '1'
  tempdat$max_ES[tempdat$ScalePoint != max_es_ScalePoint[1]] <- '0'
  tempdat$ScalePoint_max_ES <- max_es_ScalePoint[1]

  max_es <- max(abs(tempdat$d)) #find max absolute value of ES (in any direction) from each curve
  max_es_row <- which(tempdat$abs_d == max_es)[1]
  max_es_ScalePoint <- tempdat$ScalePoint[max_es_row]
  tempdat$biggest_ES[tempdat$ScalePoint == max_es_ScalePoint[1]] <- '1'
  tempdat$biggest_ES[tempdat$ScalePoint != max_es_ScalePoint[1]] <- '0'
  tempdat$ScalePoint_biggest_ES <- max_es_ScalePoint[1]
  
  mean_es <- mean(tempdat$d) #calcualte mean ES from each curve, to know its overall direction
  # get max effect size in the same direction as the mean ES:
  if (mean_es < 0) {
    max_es_md <- max(abs(tempdat[tempdat$d < 0, ]$d)) #if overall negative, pick the max abs negative ES
    }
  else {
    max_es_md <- max(abs(tempdat[tempdat$d > 0, ]$d)) #if overall negative, pick the max abs positive ES
  }
  max_es_md_row <- which(tempdat$abs_d == max_es_md)[1]
  max_es_md_ScalePoint <- tempdat$ScalePoint[max_es_md_row]
  
  tempdat$biggest_meandir[tempdat$ScalePoint == max_es_md_ScalePoint[1]] <- '1'
  tempdat$biggest_meandir[tempdat$ScalePoint != max_es_md_ScalePoint[1]] <- '0'
  tempdat$ScalePoint_biggest_meandir <- max_es_md_ScalePoint[1]

  if (i == 1){
    newdat <- tempdat
  }
  else {
    newdat <- rbind(newdat,tempdat)
  }
}

names(newdat)

# Descriptions of the new columns:

# "max_ES"            - "1" denotes the max absolute value of ES (in positive direction) from each curve
# "biggest_ES"        - "1" denotes the max absolute value of ES (in any direction) from each curve 
# "biggest_meandir"   - "1" denotes the max absolute value of ES (in any direction) in the same direction as the mean ES from each curve

# "ScalePoint_max_ES"         -  contains the name of a ScalePoints (test cue location) matching the max_ES from each curve
# "ScalePoint_biggest_ES"     - contains the name of a ScalePoints (test cue location) matching the biggest_ES from each curve
# "ScalePoint_biggest_meandir"- contains the name of a ScalePoints (test cue location) matching the max_ES from each curve

#View(newdat)
tabyl(newdat, ScalePoint)
tabyl(newdat, ScalePoint_max_ES)
tabyl(newdat, ScalePoint_biggest_ES)
tabyl(newdat, ScalePoint_biggest_meandir)

write.csv(newdat, "data/MA_dataset_with_ES_subsets.csv", row.names = FALSE)
```


###############################################################################

NEXT RUN: "STEP_02_summary.Rmd"
