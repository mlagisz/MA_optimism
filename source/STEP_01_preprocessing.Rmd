---
title: "preprocessing"
author: "ML"
date: "08/27/2019"
output: html_document
---

# MA of cognitive bias (optimism) in animal studies.

## STEP_01 - preprocessing data

### Setup R
```{r setup}
writeLines(capture.output(sessionInfo()), "source/sessionInfo.txt") #save session info into a text file

options(scipen=100)

#install.packages("car", "meta", "metafor", "ape", "dplyr", "ggplot2", "scales", "cowplot")
#install.packages("devtools","fulltxt")
#library(readxl)
#update.packages("fulltxt")
#library(fulltext)
#devtools::install_github("ropensci/rotl", dependencies = TRUE, build_vignette=TRUE)
#install.packages("rotl")
#library(rotl)
#library(ape)
#library(phytools)
#library(dplyr)
#library(tidyr)
#library(ggplot2)
#library(scales)
#library(easyGgplot2)
#library(cowplot)

pacman::p_load(tidyverse, magrittr, fulltext, rotl, ape, dplyr, tidyr, readxl, janitor) #load packages
source("functions/functions.R") #load custom functions
```

### Load MA dataset
                                        
```{r load MA extractions dataset, eval=TRUE}                                          
#load data from Excel file:
dat <- read_excel("./data/MA_dataset_2019_updated.xlsx", sheet = 1, range="A1:BH520")
dim(dat) #519 60
names(dat)
tail(dat)
str(dat) 

#exclude data points marked as "Y" in the Exclude column: 
dat %>% tabyl(Exclude) #60 rows to be removed (usually missing values at cues or wrong experimental design)
dat %>% filter(is.na(Exclude)) -> dat
dim(dat) #459 60
dat %>% dplyr::select(-c(Exclude)) -> dat#get rid of empty column

#check for missing data:
unlist(lapply(dat, function(x) sum(is.na(x)))) #see how many NA per numerical column, no missing means or SD or SE or N

get_dupes(dat, EffectID) #there are 5 pairs of duplicated EffectID, fix
dat$EffectID <- as.factor(paste("ES", 1:nrow(dat), sep="_")) #fix column with ES ID (unique ID for effect sizes)

dat <- droplevels(dat) #get rid of redundant factor levels
```

### Initial data checks

```{r data check, eval=TRUE}  
#study designs
dat %>% tabyl(WithinBetween)
dat %>% tabyl(StudyDesign, WithinBetween)

#which means are already logit-transformed?
table(dat$DataScale) #6 data points on logit scale
dat[which(dat$DataScale == "logit"), ]

#find rows with means or SD = 0 for natural scale proportion data
dat[which(dat$Better == 0 & dat$DataScale == "natural"), ] #2 rows
dat[which(dat$BetterSD == 0 & dat$DataScale == "natural"), ] #n5 rows
dat[which(dat$Worse == 0 & dat$DataScale == "natural"), ] #10 rows
dat[which(dat$WorseSD == 0 & dat$DataScale == "natural"), ] #15 rows
sort(dat[which(dat$Worse < 0.5 & dat$DataScale == "natural"), ]$Worse)
dat$Worse[which(dat$Worse == 0 & dat$DataScale == "natural")] <- 0.01 #substitute the 0 values with 0.01
sort(dat[which(dat$WorseSD < 0.5 & dat$DataScale == "natural"), ]$WorseSD)
dat$WorseSD[which(dat$WorseSD == 0 & dat$DataScale == "natural")] <- 0.01 #substitute the 0 values with 0.01

#find rows with means proportions > 1 (more than 100%) for the proportion data subset
dat[which(dat$Better >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ] #32 rows
dat[which(dat$Worse >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ] #35 rows
sort(dat[which(dat$Worse > 0.9 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ]$Worse) #largest value before 1 is 99.16
dat$Worse[which(dat$Worse >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion")] <- 0.96 #substitute the >1 value with 0.992
sort(dat[which(dat$Better > 0.9 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ]$Better) #largest value before 1 is 98.33
dat$Better[which(dat$Better >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion")] <- 0.994 #substitute the >1 value with 0.992

#check again for rows with mean proportion > 1 (more than 100%) for the proportion data
dat[which(dat$Better >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ] #0 rows
dat[which(dat$Worse >= 1 & dat$DataScale == "natural" & dat$MeasureType == "proportion"), ]  #0 rows
```


### Caluclate effect sizes (ES)

The outcome measures are either letencies or proportions (percent or logit). Need to use different functions for calculating effect sizes.

```{r calculate ES, eval=TRUE}  
 dat %>% tabyl(MeasureType, DataScale) # 201 "latency" (natural), 248 "proportion", and 10 "logit proportion"

#subset the original dataframe by outcome measure type and recalculate means and variances to the same scale using custom functions for specific data types and scales

## latency data subset
 dat_lat <- dplyr::filter(dat, MeasureType == "latency") #subset latency data
 dim(dat_lat) 
 dat_lat2 <- dat_lat %>% calc_ES_latency(Worse, WorseSD, WorseN, Better, BetterSD, BetterN, WithinBetween, adjusted=TRUE, type="lnorm") #calcualte ES (d = Hedges g) and reverse its sign
 
 dim(dat_lat2)
 hist(dat_lat2$d)
 hist(dat_lat2$Vd)
 #dplyr::filter(dat_lat2, d < -8)[, 1:6]
 #dplyr::filter(dat_lat2, d < -8)[, 46:67]
 
## logit of proportion data subset
 dat_prop_logit <- dplyr::filter(dat, MeasureType == "proportion" & DataScale == "logit") #subset proportion data (percentage) on logit scale
 dim(dat_prop_logit)
 table(dat_prop_logit$WithinBetween) #by stydy design type: 6 between
 dat_prop_logit2 <- dat_prop_logit %>% calc_ES_proportion(Worse, WorseSD, WorseN, Better, BetterSD, BetterN, WithinBetween, adjusted=TRUE, type="logit") #calcualte ES (d = Hedges g)

 dim(dat_prop_logit2)
 hist(dat_prop_logit2$d)
 hist(dat_prop_logit2$Vd)
  
## percent data subset
 dat_prop_pct <- dplyr::filter(dat, MeasureType == "proportion" & DataScale == "natural") #subset proportion data (percentage) on natural scale
 dim(dat_prop_pct) 
 hist(dat_prop_pct$Better)
 hist(dat_prop_pct$Worse)
 hist(dat_prop_pct$BetterSD) #looks like % not proportion!!!!
 hist(dat_prop_pct$WorseSD) #looks like % not proportion!!!!
 
 table(dat_prop_pct$WithinBetween) #by stydy design type: 62 between, 58 within
 dat_prop_pct2 <- dat_prop_pct %>% calc_ES_proportion(Worse, WorseSD, WorseN, Better, BetterSD, BetterN, WithinBetween, adjusted=TRUE, type="proportion") #calcualte ES (d = Hedges g)
 
 dim(dat_prop_pct2)
 hist(dat_prop_pct2$d)
 hist(dat_prop_pct2$Vd)

#dat_lat2[1:10, 46:67]
 #dplyr::filter(dat_lat2, d < -8)[, 46:67]
 
## merge back the subsets
dat2 <- rbind(dat_lat2, dat_prop_logit2, dat_prop_pct2)
names(dat2)
dat2$d
hist(dat2$d)
hist(dat2$Vd)

dat2$Species_Latin <- as.character(dat2$ScientificName) #Make new column in the data frame and adjust species names
write.csv(dat2, "data_with_ES.csv", row.names = FALSE)

#View(dplyr::filter(dat2, d > 4)) #look at effect sizes > 4  checked against the original papers
#View(dplyr::filter(dat2, d < (-2))) #look at effect sizes <  -4  checked agains the original papers
```


```{r clean, eval=FALSE}  
# #remove odd one?
# #dat2[dat2$ArticleID == "Doyle2011b", ]
# dat2 <- dat2[dat2$ArticleID != "Doyle2011b", ] #sheep long terms stress
# dim(dat2)
```

############## TREE 

Prepare phyogenetic tree for the included species  

```{r  tree, eval=TRUE}
#The function tnrs_match_names returns a data frame that lists the Open Tree identifiers as
#well as other information to help users ensure that the taxa matched are the correct ones

names(dat2)
dat2 %>% tabyl(ScientificName)
unique(dat2$Species_Latin)

species_list <- as.character(unique(dat2$Species_Latin))
taxa <- tnrs_match_names(names = species_list) #call rotl function to find these species on OTL
#taxa
tree <- tol_induced_subtree(ott_ids= taxa[["ott_id"]])
tree
plot(tree, cex=.8, label.offset =.1, no.margin = TRUE)
str(tree) # no branch lengths
is.binary.tree(tree) #TRUE 
tree$node.label <- "" # remove
tree$tip.label <- gsub("_ott.*","", tree$tip.label) #clean up tip labels
plot(tree, cex=.8, label.offset =.1, no.margin = TRUE)
tree <- collapse.singles(tree)
intersect(unique(dat2$Species_Latin), tree$tip.label) #all are matching

### computing branch lengths
tree_bl <- compute.brlen(tree)
plot(tree_bl)
is.binary.tree(tree_bl)
is.ultrametric(tree_bl)
str(tree_bl)
#if you need to remove some species from the tree later (due to working on the subset of the data), use drop.tip function, for example:
#tree_noInsect <- drop.tip(tree, "Bombus_terrestris_audax") # remove "Bombus_terrestris_audax" from the tree

write.tree(tree_bl,file="tree_all.tre")
#tree <- read.tree(file="tree_all.tre")
```



############################## NEW adding variables for creating subsets

```{r  subsets, eval=TRUE}
dat <- read.csv("data_with_ES.csv")
table(dat$ExperimentID)
table(dat$ExperimentID, dat$Sex)
table(dat$ExperimentID, dat$Sex, dat$MeasureType) 
table(table(dat$ExperimentID, dat$Sex, dat$MeasureType))
table(table(dat$ExperimentID, dat$Sex, dat$MeasureType, dat$ComparisonCategory)) #use this to get all triplets and singlets (points from the same curve)

dat2 <- dat
#tbl_df(dat2) %>% group_by(ExperimentID, MeasureType, GroupID, ComparisonCategory, Sex) # triplets are set by indices stored in a list (107)
dat2$unique <- as.factor(paste(paste0(dat2$ExperimentID, dat2$MeasureType, dat2$GroupID, dat2$ComparisonCategory, dat2$Sex)))
uni <- levels(dat2$unique)
#uni

dat2$max_ES <- NA
dat2$biggest_ES <- NA
dat2$biggest_meandir <- NA
dat2$abs_d <- abs(dat2$d)

  
for (i in 1:length(uni)){
  
  tempdat <- dat2[dat2$unique == uni[i],] #subset by unique==uni

  #NEW: add max ES in positive direction
  max_es <- max(tempdat$d) #max absolute value of ES
  max_es_row <- which(tempdat$d == max_es)[1]
  max_es_ScalePoint = tempdat$ScalePoint[max_es_row]
  tempdat$max_ES[tempdat$ScalePoint == max_es_ScalePoint[1]] <- '1'
  tempdat$max_ES[tempdat$ScalePoint != max_es_ScalePoint[1]] <- '0'
  tempdat$ScalePoint_max <- max_es_ScalePoint[1]

    
  max_es <- max(abs(tempdat$d)) #max absolute value of ES
  max_es_row <- which(tempdat$abs_d == max_es)[1]
  max_es_ScalePoint <- tempdat$ScalePoint[max_es_row]
  tempdat$biggest_ES[tempdat$ScalePoint == max_es_ScalePoint[1]] <- '1'
  tempdat$biggest_ES[tempdat$ScalePoint != max_es_ScalePoint[1]] <- '0'
  tempdat$ScalePoint1 <- max_es_ScalePoint[1]
  
  mean_es <- mean(tempdat$d) #max effect size in the same direction as the mean ES from the triplet
  
  if (mean_es<0) {
    max_es_md <- max(abs(tempdat[tempdat$d<0,]$d))
  }
  else {
    max_es_md <- max(abs(tempdat[tempdat$d > 0,]$d))
  }
  
  max_es_md_row <- which(tempdat$abs_d == max_es_md)[1]
  max_es_md_ScalePoint <- tempdat$ScalePoint[max_es_md_row]
  
  tempdat$biggest_meandir[tempdat$ScalePoint == max_es_md_ScalePoint[1]] <- '1'
  tempdat$biggest_meandir[tempdat$ScalePoint != max_es_md_ScalePoint[1]] <- '0'
  tempdat$ScalePoint2 <- max_es_md_ScalePoint[1]


  if (i == 1){
    newdat <- tempdat
  }
  else {
    newdat <- rbind(newdat,tempdat)
  }
  
}

#View(newdat)
write.csv(newdat, "data_with_ES_subsets.csv", row.names = FALSE)
```


###############################################################################

NEXT RUN: "STEP_02_summary.Rmd"
